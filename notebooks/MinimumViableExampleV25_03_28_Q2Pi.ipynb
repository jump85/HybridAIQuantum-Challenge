{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCQ0lgG4sDUX",
        "outputId": "7de75539-d2ec-4534-8968-dd94bab252a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Perceval Quest - Naples Quantum2Pi Knights/Phase1 - Naples Quantum2Pi Knights/data\n",
            "train.csv  val.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#  Navigate to project folder\n",
        "%cd /content/drive/MyDrive/'Perceval Quest - Naples Quantum2Pi Knights'/'Phase1 - Naples Quantum2Pi Knights'/data\n",
        "\n",
        "#List files to confirm location\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2I1-zMKszvu",
        "outputId": "6d4c9304-7d6b-4ecb-ce24-33f20f64fa14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting perceval-quandela\n",
            "  Downloading perceval_quandela-0.13.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: sympy~=1.12 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (1.13.1)\n",
            "Requirement already satisfied: numpy<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (2.0.2)\n",
            "Requirement already satisfied: scipy~=1.13 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (1.14.1)\n",
            "Requirement already satisfied: tabulate~=0.9 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (0.9.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (3.10.0)\n",
            "Collecting exqalibur~=0.7.0 (from perceval-quandela)\n",
            "  Downloading exqalibur-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (495 bytes)\n",
            "Requirement already satisfied: multipledispatch<2 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (5.29.4)\n",
            "Collecting drawsvg>=2.0 (from perceval-quandela)\n",
            "  Downloading drawsvg-2.4.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: requests<3 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (2.32.3)\n",
            "Requirement already satisfied: networkx~=3.1 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (3.4.2)\n",
            "Collecting latexcodec<4 (from perceval-quandela)\n",
            "  Downloading latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: platformdirs<5 in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (4.3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from perceval-quandela) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->perceval-quandela) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3->perceval-quandela) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3->perceval-quandela) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3->perceval-quandela) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3->perceval-quandela) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy~=1.12->perceval-quandela) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4->perceval-quandela) (1.17.0)\n",
            "Downloading perceval_quandela-0.13.0-py3-none-any.whl (884 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m884.3/884.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading drawsvg-2.4.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exqalibur-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: drawsvg, latexcodec, exqalibur, perceval-quandela\n",
            "Successfully installed drawsvg-2.4.0 exqalibur-0.7.0 latexcodec-3.0.0 perceval-quandela-0.13.0\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.11/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hyperopt) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hyperopt) (1.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from hyperopt) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from hyperopt) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt) (0.10.9.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision   #torch==2.* torchvision~=0.19\n",
        "!pip install matplotlib pandas tqdm==4.67.1   #pandas==2.2.3    jupyter\n",
        "!pip install perceval-quandela #!pip install perceval-quandela==0.12.0  0.11.2\n",
        "!pip install hyperopt   # Only if you're using TPE\n",
        "!pip install sympy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import yaml  # if needed; otherwise, we define config inline\n",
        "import numpy as np\n",
        "\n",
        "# Instead of loading a YAML file, define the config dictionary inline:\n",
        "cfg = {\n",
        "    \"data\": {\n",
        "        \"train_csv\": \"data/train.csv\",\n",
        "        \"test_csv\": \"data/val.csv\",\n",
        "        \"batch_size\": 30,\n",
        "        \"shuffle\": True\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"type\": \"hybrid\",  # use quantum embedding; set to \"classical\" for baseline\n",
        "        \"fourier_features\": 50,\n",
        "        \"conv_channels\": 16,\n",
        "        \"kernel_size\": 5,\n",
        "        \"output_dim\": 30\n",
        "    },\n",
        "    \"quantum\": {\n",
        "        \"backend\": \"local\",          # use \"local\" simulation\n",
        "        \"n_modes\": 24,\n",
        "        \"n_photons\": 10,\n",
        "        \"postselect\": 3,\n",
        "        \"n_samples\": 1000,\n",
        "        \"circuit_type\": \"pdf\",       # Options: \"triangular\", \"rectangular\", \"pdf\", \"base\"\n",
        "        \"variational\": True,         # set to False for non-variational version\n",
        "        \"adaptive_sampling\": False,\n",
        "        \"cache_enabled\": True,\n",
        "        \"cache_directory\": \"results/cache\",\n",
        "        \"local_backend\": \"SLOS\",\n",
        "        \"scaleway_platform\": \"sim:sampling:2l4\",\n",
        "        \"downsample_method\": \"pca\"   # or \"bilinear\"\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"epochs\": 5,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"optimizer\": \"adam\",         # \"adam\" or \"sgd\"\n",
        "        \"device\": \"auto\"             # \"cuda\", \"cpu\", or \"auto\"\n",
        "    },\n",
        "    \"optimization\": {\n",
        "        \"enabled\": True,\n",
        "        \"method\": \"tpe\",  # \"tpe\", \"grid\", or \"optuna\"\n",
        "        \"max_evals\": 10,\n",
        "        \"space\": {\n",
        "            \"n_modes\": [6, 12, 24, 30],\n",
        "            \"n_photons\": [6, 10],\n",
        "            \"learning_rate\": [0.0001, 0.001, 0.01]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# You may also need to ensure that your \"data\" and \"results\" folders exist:\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"results\", exist_ok=True)\n",
        "os.makedirs(\"results/cache\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "WD1ZJollCGNT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxRIe6_YzzpD",
        "outputId": "06879754-1f8f-489b-c1e6-432960ba9b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"CUDA available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Get the directory of this file (training.py)\n",
        "base_dir = os.getcwd()\n",
        "# Go up one level (assuming your project structure has 'src' and 'data' as siblings)\n",
        "data_dir = os.path.join(base_dir, \"data\")\n",
        "pca_model_path = os.path.join(data_dir, \"pca_model.pkl\")\n",
        "\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, bs, optimizer, cfg):\n",
        "    \"\"\"\n",
        "    Train the model for a specified number of epochs.\n",
        "    If the model uses quantum embedding (embedding_size > 0), embed images using the BosonSampler.\n",
        "    \"\"\"\n",
        "    history = []\n",
        "    train_loss_list, train_acc_list, val_loss_list, val_acc_list = [], [], [], []\n",
        "\n",
        "\n",
        "    # Compute the downsample shape (e.g. (9, 14) if nb_parameters == 126)\n",
        "    downsample_shape = compute_downsample_shape(bs.nb_parameters)\n",
        "    print(f\"Downsample shape for images set to: {downsample_shape[0]}x{downsample_shape[1]}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, running_acc = 0.0, 0.0\n",
        "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n",
        "            if model.embedding_size:\n",
        "                # Assume batch contains a list of images if batch_size > 1.\n",
        "                images, labs = batch  # images shape: [B, 1, 28, 28]\n",
        "                # Process each image sequentially (QaaS does not handle batching well)\n",
        "                emb_list = []\n",
        "                for img in images:\n",
        "\n",
        "                    if cfg[\"quantum\"].get(\"downsample_method\", \"bilinear\") == \"pca\":\n",
        "                        # Load your pre-trained PCA model (ensure you have it saved in pca_model.pkl)\n",
        "                        with open(pca_model_path, \"rb\") as f:\n",
        "                            pca_model = pickle.load(f)\n",
        "                        # Use the PCA-based downsampling and assign to img_resized\n",
        "                        img_resized  = pca_downsample(img, pca_model)\n",
        "                    else:\n",
        "                        # Use bilinear interpolation as fallback\n",
        "                        # Using sequential embedding (remove parallel_embed call)\n",
        "                        # Add batch and channel dimensions: [1, 1, 28, 28]\n",
        "                        img_expanded = img.unsqueeze(0)  # now shape: [1, 1, 28, 28]\n",
        "                        # Resize image to computed downsample shape.  (e.g. (9,14))\n",
        "                        img_resized = F.interpolate(img_expanded, size=downsample_shape, mode='bilinear', align_corners=False)\n",
        "                        # Remove batch and channel dims: shape becomes [height, width]  resulting shape becomes [1, 9, 14]\n",
        "                        img_resized = img_resized.squeeze(0)\n",
        "                        # Now the flattened tensor has exactly downsample_shape[0]*downsample_shape[1] pixels.\n",
        "\n",
        "                    emb = bs.embed(img_resized, bs.n_samples)\n",
        "                    emb_list.append(emb)\n",
        "\n",
        "                # Stack embeddings and add the batch dimension if needed.\n",
        "                #embeddings = torch.stack(embeddings)  # shape: [B, embedding_size]\n",
        "                emb = torch.stack(emb_list)\n",
        "\n",
        "                # TODO concatenate Fourier features\n",
        "                #  # Compute Fourier features for each image and stack them:\n",
        "                # fourier_feats = [fourier_encode(img, n_features=cfg[\"model\"].get(\"fourier_features\", 50)) for img in image_list]\n",
        "                # fourier_feats = torch.stack(fourier_feats)  # shape: [B, fourier_features]\n",
        "\n",
        "                # # Concatenate along feature dimension:\n",
        "                # emb = torch.cat((emb, fourier_feats), dim=1)  # new embedding size = bs.embedding_size + fourier_features\n",
        "\n",
        "\n",
        "                # Now call the training step (ensure your model.training_step accepts batched embeddings)\n",
        "                loss, acc = model.training_step((images, labs), emb=emb)\n",
        "                #loss, acc = model.training_step(batch, emb=emb)\n",
        "            else:\n",
        "                loss, acc = model.training_step(batch)\n",
        "\n",
        "            #if step % 50 == 0:\n",
        "            tqdm.write(f\"Step {step}: loss={loss.item():.4f}, acc={acc.item():.4f}\")\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            running_loss += float(loss.detach())\n",
        "            running_acc += float(acc.detach())\n",
        "            if model.embedding_size and step % 100 == 0 and step > 0:\n",
        "                print(f\"Step {step}, Training acc: {running_acc/(step+1):.4f}, Loss: {running_loss/(step+1):.4f}\")\n",
        "        result = evaluate(model, val_loader, bs, cfg)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "        train_loss_list.append(running_loss / len(train_loader))\n",
        "        train_acc_list.append(running_acc / len(train_loader))\n",
        "        val_loss_list.append(result[\"val_loss\"])\n",
        "        val_acc_list.append(result[\"val_acc\"])\n",
        "        plot_training_metrics(train_acc_list, val_acc_list, train_loss_list, val_loss_list)\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "#TODO Integrate Fourier Features into the Data Pipeline  **\n",
        "#  Concatenate with Quantum Embedding. OR  Preprocess Images Before Feeding into the CNN.\n",
        "#fourier_feats = fourier_encode(images.squeeze(0).squeeze(0), n_features=50)  # choose 50 features\n",
        "## After obtaining the quantum embedding (emb), concatenate them:\n",
        "#emb = torch.cat((emb, fourier_feats.unsqueeze(0)), dim=1)\n",
        "#######\n",
        "#fourier_size = cfg[\"model\"].get(\"fourier_features\", 0)\n",
        "#total_embedding_size = bs.embedding_size + fourier_size if cfg[\"model\"][\"type\"] == \"hybrid\" else 0\n",
        "## When initializing HybridMnistModel:\n",
        "#net = HybridMnistModel(device=device, embedding_size=total_embedding_size)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, bs, cfg):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the validation set.\n",
        "    If quantum embedding is used, compute it for each sample.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "\n",
        "    # Compute the target downsample shape (e.g. 9x14 if bs.nb_parameters equals 126)\n",
        "    downsample_shape = compute_downsample_shape(bs.nb_parameters)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            if model.embedding_size:\n",
        "                images, labs = batch # images shape: [B, 1, 28, 28]\n",
        "                batch_embeddings = []\n",
        "                # Process each image in the batch individually:\n",
        "                for i in range(images.size(0)):\n",
        "                    img = images[i]  # shape: [1, 28, 28]\n",
        "                    # Apply downsampling based on the chosen method.\n",
        "                    if cfg[\"quantum\"].get(\"downsample_method\", \"bilinear\") == \"pca\":\n",
        "                        # Load the PCA model (ensure the path is correct)\n",
        "                        with open(pca_model_path, \"rb\") as f:\n",
        "                            pca_model = pickle.load(f)\n",
        "                        img_resized = pca_downsample(img, pca_model)\n",
        "                    else:\n",
        "                        # Use bilinear interpolation\n",
        "                        # Expand dimensions to [1, 1, 28, 28] if needed.\n",
        "                        img_expanded = img.unsqueeze(0)\n",
        "                        img_resized = F.interpolate(img_expanded, size=downsample_shape, mode='bilinear', align_corners=False)\n",
        "                        img_resized = img_resized.squeeze(0)\n",
        "\n",
        "\n",
        "                    # Now img_resized should have exactly downsample_shape[0]*downsample_shape[1] pixels.\n",
        "                    # Compute the quantum embedding.\n",
        "                    # Depending on configuration, use adaptive or standard embedding\n",
        "                    if cfg[\"quantum\"].get(\"adaptive_sampling\", False):\n",
        "                        emb = bs.adaptive_embed(img_resized, min_samples=100, max_samples=bs.n_samples, tol=1e-3)\n",
        "                    else:\n",
        "                        emb = bs.embed(img_resized, bs.n_samples)\n",
        "                    batch_embeddings.append(emb)\n",
        "                # Stack embeddings to form a tensor of shape [B, embedding_size]\n",
        "                emb_tensor = torch.stack(batch_embeddings)\n",
        "                # Pass the batch and the corresponding embeddings to the model's validation step\n",
        "                out = model.validation_step((images, labs), emb=emb_tensor)\n",
        "            else:\n",
        "                out = model.validation_step(batch)\n",
        "            outputs.append(out)\n",
        "    batch_losses = [x[\"val_loss\"] for x in outputs]\n",
        "    batch_accs = [x[\"val_acc\"] for x in outputs]\n",
        "    val_loss = torch.stack(batch_losses).mean().item()\n",
        "    val_acc = torch.stack(batch_accs).mean().item()\n",
        "    return {\"val_loss\": val_loss, \"val_acc\": val_acc}"
      ],
      "metadata": {
        "id": "pYDwCz7GhWab"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# src/boson_sampler.py\n",
        "import perceval as pcvl\n",
        "from perceval.providers import scaleway  # import Scaleway Session class if needed\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import hashlib\n",
        "import pickle\n",
        "import numpy as np\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "def merge_circuits(c1, c2):\n",
        "    \"\"\"\n",
        "    Merge two circuits by creating a new composite circuit.\n",
        "    This helper creates a new circuit with the same number of modes as c1,\n",
        "    and adds c1 and then c2 using the add() method with merge=True.\n",
        "    \"\"\"\n",
        "    composite = pcvl.Circuit(c1.m)\n",
        "    # Add the first circuit on port 0 and merge it into the composite circuit.\n",
        "    composite.add(0, c1, merge=True)\n",
        "    # Add the second circuit similarly.\n",
        "    composite.add(0, c2, merge=True)\n",
        "    return composite\n",
        "\n",
        "def compose_circuits(circuit_list):\n",
        "    \"\"\"\n",
        "    Compose a list of circuits into a single circuit using merge_circuits.\n",
        "    \"\"\"\n",
        "    composite = circuit_list[0]\n",
        "    for circ in circuit_list[1:]:\n",
        "        composite = merge_circuits(composite, circ)\n",
        "    return composite\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class InterferometerBuilder(ABC):\n",
        "    @abstractmethod\n",
        "    def create_circuit(self, m: int, parameters: list[float] = None) -> pcvl.Circuit:\n",
        "        \"\"\"Return a Perceval circuit for m modes given a list of phase parameters.\"\"\"\n",
        "        pass\n",
        "\n",
        "def safe_param(i, params):\n",
        "    # If i is within bounds, return the parameter; otherwise, default to 0.\n",
        "    return params[i] if i < len(params) else 0\n",
        "\n",
        "class TriangularInterferometerBuilder(InterferometerBuilder):\n",
        "    def create_circuit(self, m: int, parameters: list[float] = None) -> pcvl.Circuit:\n",
        "        if parameters is None:\n",
        "            # Use symbolic parameters: for a triangular mesh you need m*(m-1) parameters.\n",
        "            parameters = [pcvl.P(f\"phi_{i}\") for i in range(m * (m - 1))]\n",
        "        # Build the circuit using the local variable m and the correct variable name (parameters)\n",
        "        return pcvl.GenericInterferometer(m, lambda idx: (\n",
        "            pcvl.BS()\n",
        "            .add(0, pcvl.PS(safe_param(2 * idx, parameters)))\n",
        "            .add(0, pcvl.BS())\n",
        "            .add(0, pcvl.PS(safe_param(2 * idx + 1, parameters)))\n",
        "        ))\n",
        "\n",
        "\n",
        "class RectangularInterferometerBuilder(InterferometerBuilder):\n",
        "    def create_circuit(self, m: int, parameters: list[float] = None) -> pcvl.Circuit:\n",
        "        if parameters is None:\n",
        "            parameters = [pcvl.P(f\"phi_rect_{i}\") for i in range(m * (m - 1))]\n",
        "        # For demonstration, we build two layers sequentially.\n",
        "        # (Note: In practice, you’d implement the proper rectangular mesh.)\n",
        "        layer1 = pcvl.GenericInterferometer(m, lambda idx: (\n",
        "            pcvl.BS().add(0, pcvl.PS(safe_param(2 * idx, parameters)))\n",
        "        ))\n",
        "        layer2 = pcvl.GenericInterferometer(m, lambda idx: (\n",
        "            pcvl.BS().add(0, pcvl.PS(safe_param(2 * idx + 1, parameters)))\n",
        "        ))\n",
        "        return merge_circuits(layer1, layer2)\n",
        "\n",
        "\n",
        "\n",
        "class PdfInterferometerBuilder:\n",
        "    def create_circuit(self, m: int, parameters=None) -> pcvl.Circuit:\n",
        "        \"\"\"\n",
        "        Create a circuit as described in Quantum_Circuits.pdf.\n",
        "        Expects 'parameters' to be either:\n",
        "          - A dict with keys 'theta', 'alpha', and 'beta'\n",
        "            where 'theta' is a list of three phase values,\n",
        "                  'alpha' is a list of two values for the first BS block, and\n",
        "                  'beta' is a list of two values for the second BS block,\n",
        "          - Or a list of at least 7 numbers, which will be interpreted as\n",
        "            theta[0:3], alpha[0:2], beta[0:2].\n",
        "        \"\"\"\n",
        "        if parameters is None:\n",
        "            parameters = {\n",
        "                'theta': [pcvl.P(\"theta1\"), pcvl.P(\"theta2\"), pcvl.P(\"theta3\")],\n",
        "                'alpha': [0.1, 0.2],\n",
        "                'beta': [0.3, 0.4]\n",
        "            }\n",
        "        elif not isinstance(parameters, dict):\n",
        "            # Assume parameters is a list; require at least 7 values\n",
        "            if len(parameters) < 7:\n",
        "                raise ValueError(\"PDFInterferometerBuilder requires at least 7 parameters when provided as a list.\")\n",
        "            parameters = {\n",
        "                'theta': parameters[:3],\n",
        "                'alpha': parameters[3:5],\n",
        "                'beta': parameters[5:7]\n",
        "            }\n",
        "\n",
        "        def U_block(m, theta):\n",
        "            sub = pcvl.Circuit(m)\n",
        "            # Apply a phase shift on each mode with the same theta.\n",
        "            for mode in range(m):\n",
        "                sub.add(mode, pcvl.PS(theta), merge=True)\n",
        "            return sub\n",
        "\n",
        "        def BS_block(m, bs_params):\n",
        "            sub = pcvl.Circuit(m)\n",
        "            # For demonstration, add a beam splitter between modes 0 and 1.\n",
        "            sub.add((0, 1), pcvl.BS(), merge=True)\n",
        "            return sub\n",
        "\n",
        "        circuit = pcvl.Circuit(m)\n",
        "        circuit = circuit.add(0, U_block(m, parameters['theta'][0]), merge=True)\n",
        "        circuit = circuit.add(0, BS_block(m, parameters['alpha']), merge=True)\n",
        "        circuit = circuit.add(0, U_block(m, parameters['theta'][1]), merge=True)\n",
        "        circuit = circuit.add(0, BS_block(m, parameters['beta']), merge=True)\n",
        "        circuit = circuit.add(0, U_block(m, parameters['theta'][2]), merge=True)\n",
        "        return circuit\n",
        "\n",
        "\n",
        "class BaseInterferometerBuilder:\n",
        "    def create_circuit(self, m: int, parameters: list = None) -> pcvl.Circuit:\n",
        "        \"\"\"\n",
        "        Create an alternative interferometer circuit using a simplified rectangular layout.\n",
        "        If no parameters are provided, generate a list of symbolic parameters of length m*(m-1).\n",
        "        The lambda uses these parameters in pairs.\n",
        "        \"\"\"\n",
        "        # Compute the expected total number of phases (should be m*(m-1))\n",
        "        total_needed = m * (m - 1)\n",
        "        if parameters is None:\n",
        "            parameters = [pcvl.P(f\"phi_{i}\") for i in range(total_needed)]\n",
        "\n",
        "        def base_lambda(i):\n",
        "            # Use the actual length of parameters to avoid index errors.\n",
        "            # Each pair of parameters is used for one layer.\n",
        "            num_pairs = len(parameters) // 2\n",
        "            j = i % num_pairs\n",
        "            return (pcvl.BS()\n",
        "                    .add(0, pcvl.PS(parameters[2 * j]), merge=True)\n",
        "                    .add(0, pcvl.BS(), merge=True)\n",
        "                    .add(0, pcvl.PS(parameters[2 * j + 1]), merge=True))\n",
        "\n",
        "        circuit = pcvl.GenericInterferometer(m, base_lambda)\n",
        "        return circuit\n",
        "\n",
        "\n",
        "\n",
        "class ConvolutionalInterferometerBuilder:\n",
        "    def __init__(self, filterA, filterB, image_size=(28, 28)):\n",
        "        \"\"\"\n",
        "        Initialize with filterA and filterB (numpy arrays) and the expected image size.\n",
        "        For example, use filterA and filterB of shape (6,5) to yield 23x24=552 outputs.\n",
        "        \"\"\"\n",
        "        self.filterA = filterA\n",
        "        self.filterB = filterB\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def create_circuit(self, m: int, parameters: list = None) -> pcvl.Circuit:\n",
        "        \"\"\"\n",
        "        Constructs a full circuit that interleaves trainable U-blocks with convolutional encoding blocks.\n",
        "        The conv encoding blocks use parameters for α and β.\n",
        "        If 'parameters' is None, symbolic parameters are used for the conv blocks.\n",
        "        Otherwise, 'parameters' is assumed to be a list containing [alpha_flat, beta_flat] concatenated.\n",
        "        \"\"\"\n",
        "        expected_length = m * (m - 1)  # For a triangular interferometer\n",
        "\n",
        "        # Expected output sizes from valid convolution:\n",
        "        filterA_shape = self.filterA.shape  # e.g., (6,5)\n",
        "        filterB_shape = self.filterB.shape  # e.g., (6,5)\n",
        "        alpha_rows = self.image_size[0] - filterA_shape[0] + 1\n",
        "        alpha_cols = self.image_size[1] - filterA_shape[1] + 1\n",
        "        beta_rows  = self.image_size[0] - filterB_shape[0] + 1\n",
        "        beta_cols  = self.image_size[1] - filterB_shape[1] + 1\n",
        "        alpha_out_len = alpha_rows * alpha_cols  # e.g., 23*24 = 552\n",
        "        beta_out_len  = beta_rows * beta_cols\n",
        "\n",
        "        if parameters is None:\n",
        "            # Use symbolic parameters for the conv blocks.\n",
        "            alpha_params = [pcvl.P(f\"alpha_{i}\") for i in range(alpha_out_len)]\n",
        "            beta_params  = [pcvl.P(f\"beta_{j}\") for j in range(beta_out_len)]\n",
        "        else:\n",
        "            # Split the provided parameter list into alpha and beta portions.\n",
        "            alpha_params = parameters[:alpha_out_len]\n",
        "            beta_params  = parameters[alpha_out_len:alpha_out_len+beta_out_len]\n",
        "\n",
        "        # Truncate the conv parameters to the expected length.\n",
        "        if len(alpha_params) > expected_length:\n",
        "            alpha_params = alpha_params[:expected_length]\n",
        "        if len(beta_params) > expected_length:\n",
        "            beta_params = beta_params[:expected_length]\n",
        "        # Optionally, pad if fewer (not likely with full convolution)\n",
        "        if len(alpha_params) < expected_length:\n",
        "            alpha_params.extend([0.0]*(expected_length - len(alpha_params)))\n",
        "        if len(beta_params) < expected_length:\n",
        "            beta_params.extend([0.0]*(expected_length - len(beta_params)))\n",
        "\n",
        "        # Build sub-circuits:\n",
        "        U1 = pcvl.GenericInterferometer(m, lambda idx: pcvl.BS() // pcvl.PS(pcvl.P(f\"theta1_{idx}\")), shape=pcvl.InterferometerShape.TRIANGLE)\n",
        "        U2 = pcvl.GenericInterferometer(m, lambda idx: pcvl.BS() // pcvl.PS(pcvl.P(f\"theta2_{idx}\")), shape=pcvl.InterferometerShape.TRIANGLE)\n",
        "        U3 = pcvl.GenericInterferometer(m, lambda idx: pcvl.BS() // pcvl.PS(pcvl.P(f\"theta3_{idx}\")), shape=pcvl.InterferometerShape.TRIANGLE)\n",
        "        Conv1 = pcvl.GenericInterferometer(m, lambda idx: pcvl.BS() // pcvl.PS(alpha_params[idx]), shape=pcvl.InterferometerShape.TRIANGLE)\n",
        "        Conv2 = pcvl.GenericInterferometer(m, lambda idx: pcvl.BS() // pcvl.PS(beta_params[idx]), shape=pcvl.InterferometerShape.TRIANGLE)\n",
        "\n",
        "        # Compose the full circuit by merging the sub-circuits.\n",
        "        full_circuit = compose_circuits([U1, Conv1, U2, Conv2, U3])\n",
        "        return full_circuit\n",
        "\n",
        "\n",
        "\n",
        "    def compute_conv_parameters(self, image_tensor: np.ndarray) -> list:\n",
        "        \"\"\"\n",
        "        Given an input image (as a numpy array), compute the convolution outputs for filterA and filterB,\n",
        "        normalize them to the [0, 2π] range, and return the concatenated phase parameter list.\n",
        "        If the input is 1D, reshape it to 2D using a factorization of its length.\n",
        "        \"\"\"\n",
        "        if image_tensor.ndim != 2:\n",
        "            if image_tensor.ndim == 1:\n",
        "                # Import the downsample shape helper from utils\n",
        "                from utils import compute_downsample_shape\n",
        "                h, w = compute_downsample_shape(image_tensor.size)\n",
        "                image_tensor = image_tensor.reshape((h, w))\n",
        "            else:\n",
        "                raise ValueError(\"Input image_tensor must be 2D after squeezing.\")\n",
        "\n",
        "        # Compute valid convolutions.\n",
        "        alpha = convolve2d(image_tensor, self.filterA, mode=\"valid\")\n",
        "        beta  = convolve2d(image_tensor, self.filterB, mode=\"valid\")\n",
        "        # Flatten and normalize: here we use modulo 1 and scale to 2π (adjust normalization as needed)\n",
        "        alpha_flat = (alpha.flatten() % 1.0) * 2 * np.pi\n",
        "        beta_flat  = (beta.flatten() % 1.0) * 2 * np.pi\n",
        "        param_vec = np.concatenate([alpha_flat, beta_flat])\n",
        "        return param_vec.tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BosonSampler:\n",
        "    def __init__(self, m: int, n: int, postselect: int = None, backend: str = \"SLOS\", session=None, builder: InterferometerBuilder = None, cache_enabled=False, cache_directory=\"results/cache\"):\n",
        "        \"\"\"\n",
        "        Photonic boson sampler for embedding data.\n",
        "        :param m: number of modes\n",
        "        :param n: number of photons (n <= m)\n",
        "        :param postselect: minimum detected photons for a valid output (defaults to n if None)\n",
        "        :param backend: local simulation backend name (ignored if session is provided)\n",
        "        :param session: optional Perceval session for remote execution (Scaleway)\n",
        "        :param builder: An instance of InterferometerBuilder to build the circuit.\n",
        "        \"\"\"\n",
        "        assert n <= m, \"Photons n must be <= modes m\"\n",
        "        self.m = m\n",
        "        self.n = n\n",
        "        self.postselect = postselect if postselect is not None else n\n",
        "        assert self.postselect <= n, \"Postselect cannot exceed number of photons\"\n",
        "        self.backend = backend\n",
        "        self.session = session  # If not None, use remote session for simulation\n",
        "        # If no builder is given, use the triangular mesh as default.\n",
        "        if builder is None:\n",
        "            self.builder = TriangularInterferometerBuilder()\n",
        "        else:\n",
        "            self.builder = builder\n",
        "\n",
        "        # Caching settings\n",
        "        self.cache_enabled = cache_enabled\n",
        "        self.cache_directory = cache_directory\n",
        "        if self.cache_enabled and not os.path.exists(self.cache_directory):\n",
        "            os.makedirs(self.cache_directory)\n",
        "\n",
        "\n",
        "    def _compute_hash(self, data_tensor):\n",
        "        # Compute a unique hash based on the tensor contents.\n",
        "        # Ensure the tensor is on CPU and convert to a numpy byte string.\n",
        "        return hashlib.md5(data_tensor.cpu().numpy().tobytes()).hexdigest()\n",
        "\n",
        "\n",
        "    @property\n",
        "    def nb_parameters(self) -> int:\n",
        "        \"\"\"Number of phase parameters available (used for embedding).\"\"\"\n",
        "        return self.m * (self.m - 1) - (self.m // 2)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def embedding_size(self) -> int:\n",
        "        \"\"\"Number of output features (possible output states after postselection).\"\"\"\n",
        "        from math import comb\n",
        "        size = 0\n",
        "        for k in range(self.postselect, self.n + 1):\n",
        "            size += comb(self.m, k)\n",
        "        return size\n",
        "\n",
        "\n",
        "    def create_circuit(self, parameters: list[float] = None) -> pcvl.Circuit:\n",
        "        \"\"\"Delegates the creation of the circuit to the builder.\"\"\"\n",
        "        return self.builder.create_circuit(self.m, parameters)\n",
        "\n",
        "    # TODO... DELTE\n",
        "    # def create_circuit(self, parameters: list[float] = None) -> pcvl.Circuit:\n",
        "    #     \"\"\"\n",
        "    #     Create an interferometer circuit with given phase parameters.\n",
        "    #     If no parameters are provided, uses symbolic placeholders.\n",
        "    #     \"\"\"\n",
        "    #     # (Use Perceval's GenericInterferometer for a mesh of beamsplitters and phase shifters)\n",
        "    #     if parameters is None:\n",
        "    #         # Use symbolic parameters if none provided\n",
        "    #         params = [pcvl.P(f\"phi_{i}\") for i in range(self.m * (self.m - 1))]\n",
        "    #         # Duplicate each parameter for paired phase shifters\n",
        "    #         #params = [val for p in parameters for val in (p, p)]\n",
        "    #     else:\n",
        "    #         params = parameters\n",
        "    #     return pcvl.GenericInterferometer(self.m, lambda idx: (\n",
        "    #                 pcvl.BS()  # 50:50 beam splitter\n",
        "    #                 .add(0, pcvl.PS(params[2*idx]))   # phase shifter, then another BS\n",
        "    #                 .add(0, pcvl.BS())\n",
        "    #                 .add(0, pcvl.PS(params[2*idx+1]))\n",
        "    #            ))\n",
        "\n",
        "    def prepare_processor(self, processor: pcvl.Processor, parameters: list[float]):\n",
        "        \"\"\"Configure the given processor with the circuit, input state, and detection filters.\"\"\"\n",
        "        processor.set_circuit(self.create_circuit(parameters))\n",
        "        processor.min_detected_photons_filter(self.postselect)  # enforce minimum detected photons\n",
        "        #processor.thresholded_output(True)   # <- DEPRECATED # use threshold detectors (no photon-number resolution)\n",
        "\n",
        "\n",
        "        from perceval.components import Detector\n",
        "        for mode in range(self.m):\n",
        "            processor.add(mode, Detector.threshold())\n",
        "\n",
        "\n",
        "\n",
        "        # Prepare an input state with n photons evenly spaced in m modes (e.g., [1,0,1,0,...] for 2 photons)\n",
        "        input_state = [0] * self.m\n",
        "        if self.n > 0:\n",
        "            step = max(1, self.m // self.n)\n",
        "            #step = self.m // self.n\n",
        "            for i in range(self.n):\n",
        "                input_state[i * step] = 1\n",
        "        processor.with_input(pcvl.BasicState(input_state))\n",
        "\n",
        "    def run(self, parameters: list[float], n_samples: int):\n",
        "        \"\"\"\n",
        "        Run the boson sampler circuit with given phase parameters, sample n_samples times.\n",
        "        Returns a Perceval BSDistribution (a dict-like object of outcome probabilities).\n",
        "        \"\"\"\n",
        "        if self.session is not None:\n",
        "            # Build a remote processor via the session (e.g., Scaleway QPU or simulator)\n",
        "            proc = self.session.build_remote_processor()            # Remote QPU simulation\n",
        "        else:\n",
        "            proc = pcvl.Processor(self.backend, self.m)             # Local simulation backend\n",
        "        # Configure the processor with the circuit and input state\n",
        "        self.prepare_processor(proc, parameters)\n",
        "        # Use Perceval's sampling algorithm (Sampler) to run the circuit\n",
        "        sampler = pcvl.algorithm.Sampler(proc, max_shots_per_call=n_samples)\n",
        "        job = sampler.probs(n_samples)  # run sampling\n",
        "        # If job has the method get_results(), use it; otherwise, assume job is already the result.\n",
        "        if hasattr(job, \"get_results\"):\n",
        "            result = job.get_results() # or job.execute_sync() depending on your workflow\n",
        "        else:\n",
        "            result = job\n",
        "\n",
        "        return result  # BSDistribution of outcomes\n",
        "\n",
        "\n",
        "    def _distribution_to_feature_vector(self, distribution):\n",
        "        \"\"\"\n",
        "        Convert a distribution dictionary into a fixed-length feature vector.\n",
        "        \"\"\"\n",
        "        feature_vec = torch.zeros(self.embedding_size)\n",
        "        state_list = self._generate_all_output_states()  # list of BasicState outcomes in canonical order\n",
        "        for i, state in enumerate(state_list):\n",
        "            feature_vec[i] = distribution.get(state, 0.0)\n",
        "        return feature_vec\n",
        "\n",
        "    def adaptive_embed(self, data_tensor, min_samples=100, max_samples=None, tol=1e-3):\n",
        "        \"\"\"\n",
        "        Compute the quantum embedding adaptively.\n",
        "        Starts with min_samples and doubles until the feature vector converges\n",
        "        (norm difference < tol) or until max_samples is reached.\n",
        "        \"\"\"\n",
        "        if max_samples is None:\n",
        "            max_samples = self.n_samples  # default maximum samples from config\n",
        "\n",
        "        prev_embedding = None\n",
        "        samples = min_samples\n",
        "        while samples <= max_samples:\n",
        "            if hasattr(self, \"params\"):\n",
        "                distribution = self.run(n_samples=samples)\n",
        "            else:\n",
        "                distribution = self.run(parameters=[], n_samples=samples)\n",
        "            current_embedding = self._distribution_to_feature_vector(distribution)\n",
        "            if prev_embedding is not None:\n",
        "                diff = torch.norm(current_embedding - prev_embedding).item()\n",
        "                if diff < tol:\n",
        "                    print(f\"Adaptive sampling converged with {samples} samples.\")\n",
        "                    return current_embedding\n",
        "            prev_embedding = current_embedding\n",
        "            samples *= 2\n",
        "        print(f\"Adaptive sampling reached max samples: {max_samples}\")\n",
        "        return prev_embedding\n",
        "\n",
        "    # def adaptive_embed(self, data_tensor, min_samples=100, max_samples=None, tol=1e-3):\n",
        "    #     \"\"\"\n",
        "    #     Compute the quantum embedding adaptively.\n",
        "    #     It computes a feature vector (from the output distribution) and doubles the sample count\n",
        "    #     until the difference between successive feature vectors is below tol or until max_samples is reached.\n",
        "    #     \"\"\"\n",
        "    #     if max_samples is None:\n",
        "    #         max_samples = self.n_samples  # default maximum samples from config\n",
        "\n",
        "    #     # Prepare phase parameters (same as in embed()):\n",
        "    #     flat = data_tensor.flatten()\n",
        "    #     if flat.shape[0] > self.nb_parameters:\n",
        "    #         raise ValueError(\"Input tensor too large for the number of modes/photons\")\n",
        "    #     phases = torch.zeros(self.nb_parameters)\n",
        "    #     phases[:flat.shape[0]] = flat\n",
        "    #     phase_list = (phases * 2 * torch.pi).tolist()\n",
        "\n",
        "    #     prev_feature = None\n",
        "    #     samples = min_samples\n",
        "    #     while samples <= max_samples:\n",
        "    #         # Run the circuit with the computed phase_list.\n",
        "    #         distribution = self.run(phase_list, n_samples=samples)\n",
        "    #         # Convert distribution to a feature vector:\n",
        "    #         feature_vec = torch.zeros(self.embedding_size)\n",
        "    #         state_list = self._generate_all_output_states()  # list of BasicState outcomes in canonical order\n",
        "    #         for i, state in enumerate(state_list):\n",
        "    #             feature_vec[i] = distribution.get(state, 0.0)\n",
        "    #         # Compare with the previous feature vector.\n",
        "    #         if prev_feature is not None:\n",
        "    #             diff = torch.norm(feature_vec - prev_feature).item()\n",
        "    #             print(f\"Adaptive sampling: {samples} samples, diff = {diff:.6f}\")\n",
        "    #             if diff < tol:\n",
        "    #                 print(f\"Adaptive sampling converged with {samples} samples.\")\n",
        "    #                 return feature_vec\n",
        "    #         prev_feature = feature_vec\n",
        "    #         samples *= 2\n",
        "    #     print(f\"Adaptive sampling reached max samples: {max_samples}\")\n",
        "    #     return prev_feature\n",
        "\n",
        "\n",
        "    # TODO delete\n",
        "    # def adaptive_embed(self, data_tensor, min_samples=100, max_samples=None, tol=1e-3):\n",
        "    #     \"\"\"\n",
        "    #     Compute the quantum embedding adaptively.\n",
        "    #     Starts with min_samples and doubles until the embedding converges\n",
        "    #     (difference less than tol) or until max_samples is reached.\n",
        "    #     \"\"\"\n",
        "    #     if max_samples is None:\n",
        "    #         max_samples = self.n_samples  # default maximum samples from config\n",
        "\n",
        "    #     prev_embedding = None\n",
        "    #     samples = min_samples\n",
        "    #     while samples <= max_samples:\n",
        "    #         current_embedding = self.run(parameters=[], n_samples=samples)\n",
        "    #         if prev_embedding is not None:\n",
        "    #             diff = torch.norm(current_embedding - prev_embedding).item()\n",
        "    #             if diff < tol:\n",
        "    #                 print(f\"Adaptive sampling converged with {samples} samples.\")\n",
        "    #                 return current_embedding\n",
        "    #         prev_embedding = current_embedding\n",
        "    #         samples *= 2\n",
        "    #     print(f\"Adaptive sampling reached max samples: {max_samples}\")\n",
        "    #     return prev_embedding\n",
        "\n",
        "\n",
        "\n",
        "    def embed(self, data_tensor, n_samples: int):\n",
        "        \"\"\"\n",
        "        Embed an input tensor (image data) into a quantum feature vector.\n",
        "        The input tensor values (assumed in [0,1]) are mapped to phase shifts.\n",
        "        :param data_tensor: input torch tensor with values in [0,1]\n",
        "        :param n_samples: number of samples for Monte Carlo estimation (ignored if using an analytic mode)\n",
        "        :return: torch tensor of shape (embedding_size,) with estimated output probabilities.\n",
        "        \"\"\"\n",
        "\n",
        "        # If caching is enabled, try to load the cached embedding.\n",
        "        if getattr(self, \"cache_enabled\", False):\n",
        "            key = hashlib.md5(data_tensor.cpu().numpy().tobytes()).hexdigest()\n",
        "            cache_file = os.path.join(self.cache_directory, f\"{key}.pkl\")\n",
        "            if os.path.exists(cache_file):\n",
        "                print(\"Loading embedding from cache.\")\n",
        "                with open(cache_file, \"rb\") as f:\n",
        "                    return pickle.load(f)\n",
        "\n",
        "\n",
        "     # Check if using convolutional embedding.\n",
        "        if isinstance(self.builder, ConvolutionalInterferometerBuilder):\n",
        "            # Convert data_tensor to numpy array (assume shape (1,28,28) or (28,28))\n",
        "            img_np = data_tensor.squeeze().cpu().numpy()\n",
        "            phase_list = self.builder.compute_conv_parameters(img_np)\n",
        "        else:\n",
        "            # Compute embedding if not cached.\n",
        "            flat = data_tensor.flatten()\n",
        "            if flat.shape[0] > self.nb_parameters:\n",
        "                raise ValueError(\"Input tensor too large for the number of modes/photons\")\n",
        "            # Prepare phase parameters: pad/truncate to fill the interferometer.\n",
        "            # Pad or truncate the phases vector to fill all required phase shifters\n",
        "            #self.nb_parameters == phase_count = self.m * (self.m - 1)  # number of phase parameters needed for full interferometer\n",
        "            phases = torch.zeros(self.nb_parameters)\n",
        "            # Use data values (scaled 0-1) for the first part of phases, remaining stay 0\n",
        "            phases[:flat.shape[0]] = flat\n",
        "            # Scale phases from [0,1] to [0, 2π] as phase shifts\n",
        "            phase_list = (phases * 2 * torch.pi).tolist()\n",
        "\n",
        "\n",
        "        # For variational sampler, self.run expects only n_samples.\n",
        "        if hasattr(self, \"params\"):\n",
        "            distribution = self.run(n_samples=n_samples)\n",
        "        else:\n",
        "            distribution = self.run(parameters=phase_list, n_samples=n_samples)\n",
        "\n",
        "        ## Run the circuit and get the output distribution\n",
        "        ##distribution = self.run(phase_list, n_samples)\n",
        "\n",
        "\n",
        "        # Convert distribution to a fixed-length probability vector\n",
        "        #feature_vec = self._distribution_to_feature_vector(distribution)\n",
        "        feature_vec = torch.zeros(self.embedding_size)\n",
        "        state_list = self._generate_all_output_states()  # list of BasicState outcomes in canonical order\n",
        "        for i, state in enumerate(state_list):\n",
        "            feature_vec[i] = distribution.get(state, 0.0)\n",
        "\n",
        "\n",
        "        # Save the computed embedding to cache.\n",
        "        if getattr(self, \"cache_enabled\", False):\n",
        "            with open(cache_file, \"wb\") as f:\n",
        "                pickle.dump(feature_vec, f)\n",
        "\n",
        "\n",
        "        return feature_vec\n",
        "\n",
        "    #@lru_cache(maxsize=1)\n",
        "    def _generate_all_output_states(self):\n",
        "        \"\"\"Precompute all possible output basis states (with postselection).\n",
        "        Generate and cache all possible output states (as BasicState objects).\n",
        "        \"\"\"\n",
        "        states = []\n",
        "        for k in range(self.postselect, self.n + 1):\n",
        "            from itertools import combinations\n",
        "            for ones in combinations(range(self.m), k):\n",
        "                bitstring = [1 if j in ones else 0 for j in range(self.m)]\n",
        "                states.append(pcvl.BasicState(bitstring))\n",
        "        return states\n",
        "\n",
        "    from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "    @staticmethod\n",
        "    def parallel_embed(self, image_list, n_samples, adaptive=False, min_samples=100, tol=1e-3):\n",
        "        \"\"\"\n",
        "        Compute embeddings for a list of images concurrently.\n",
        "        NOTE: This method is useful for local simulation.\n",
        "            However, QaaS (remote) does not handle batch processing well.\n",
        "            It is recommended to process images sequentially when using the remote platform.\n",
        "        Parameters:\n",
        "          image_list : List of image tensors.\n",
        "          n_samples  : Maximum number of samples for each embedding.\n",
        "          adaptive   : If True, uses adaptive_embed; otherwise uses embed.\n",
        "          min_samples: Starting sample count for adaptive sampling.\n",
        "          tol        : Tolerance for convergence in adaptive sampling.\n",
        "        Returns:\n",
        "          A list of embeddings.\n",
        "        \"\"\"\n",
        "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "            futures = []\n",
        "            for img in image_list:\n",
        "                if adaptive:\n",
        "                    futures.append(executor.submit(self.adaptive_embed, img, min_samples, n_samples, tol))\n",
        "                else:\n",
        "                    futures.append(executor.submit(self.embed, img, n_samples))\n",
        "            embeddings = [f.result() for f in futures]\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class VariationalBosonSampler(BosonSampler):\n",
        "    def __init__(self, m: int, n: int, postselect: int = None, backend: str = \"SLOS\",\n",
        "                 session=None, builder: InterferometerBuilder = None, cache_enabled=False,\n",
        "                 cache_directory=\"results/cache\", init_params=None):\n",
        "        \"\"\"\n",
        "        A variant of BosonSampler where the phase parameters are trainable.\n",
        "        \"\"\"\n",
        "        super().__init__(m, n, postselect, backend, session, builder, cache_enabled, cache_directory)\n",
        "        # Initialize trainable parameters. If not provided, initialize to small constant values.\n",
        "        if init_params is None:\n",
        "            init_params = [0.1] * self.nb_parameters\n",
        "        # Register parameters as a torch.nn.Parameter so they are updated during training.\n",
        "        self.params = nn.Parameter(torch.tensor(init_params, dtype=torch.float32))\n",
        "\n",
        "    adaptive_embed = BosonSampler.adaptive_embed\n",
        "\n",
        "\n",
        "    def create_circuit(self, parameters: list[float] = None) -> pcvl.Circuit:\n",
        "        \"\"\"\n",
        "        Overrides the create_circuit method to use trainable parameters.\n",
        "        \"\"\"\n",
        "        # Use the current trainable parameters from self.params\n",
        "        phase_list = self.params.tolist()\n",
        "        return self.builder.create_circuit(self.m, phase_list)\n",
        "\n",
        "    def run(self, n_samples: int, parameters: list[float] = None):\n",
        "        \"\"\"\n",
        "        Overrides run() to build the circuit using trainable parameters.\n",
        "        The 'parameters' argument is ignored since self.params is used.\n",
        "        \"\"\"\n",
        "        circuit = self.create_circuit()  # This uses self.params internally.\n",
        "        if self.session is not None:\n",
        "            proc = self.session.build_remote_processor()\n",
        "        else:\n",
        "            proc = pcvl.Processor(self.backend, self.m)\n",
        "        # We ignore the passed 'parameters' and use self.params instead.\n",
        "        self.prepare_processor(proc, parameters=[])\n",
        "        sampler = pcvl.algorithm.Sampler(proc, max_shots_per_call=n_samples)\n",
        "        job = sampler.probs(n_samples)\n",
        "        if hasattr(job, \"get_results\"):\n",
        "            result = job.get_results()\n",
        "        else:\n",
        "            result = job\n",
        "        return result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############\n",
        "# Enhanced BosonSampler with Caching\n",
        "# Implementing caching to avoid redundant quantum computations:\n",
        "############\n",
        "from functools import lru_cache\n",
        "import os\n",
        "import pickle\n",
        "import hashlib\n",
        "\n",
        "class CachedBosonSampler(BosonSampler):\n",
        "    \"\"\"\n",
        "    Extended BosonSampler with caching capabilities to avoid redundant computations.\n",
        "    \"\"\"\n",
        "    def __init__(self, m, n, postselect=None, session=None,\n",
        "                 cache_enabled=True, cache_size=1000,\n",
        "                 disk_cache=False, cache_dir=None):\n",
        "        super().__init__(m, n, postselect, session)\n",
        "        self.cache_enabled = cache_enabled\n",
        "        self.disk_cache = disk_cache\n",
        "        self.cache_dir = cache_dir\n",
        "\n",
        "        if disk_cache and cache_dir:\n",
        "            os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        # Use the wrapper function to apply caching\n",
        "        if cache_enabled:\n",
        "            self.embed = self._cached_embed(self.embed, maxsize=cache_size)\n",
        "\n",
        "    def _cached_embed(self, func, maxsize=1000):\n",
        "        \"\"\"Apply in-memory caching with LRU strategy.\"\"\"\n",
        "        memory_cache = lru_cache(maxsize=maxsize)(func)\n",
        "\n",
        "        def wrapped_func(t, n_sample):\n",
        "            # Convert tensor to tuple for hashing\n",
        "            t_tuple = tuple(t.reshape(-1).tolist())\n",
        "\n",
        "            # Check disk cache if enabled\n",
        "            if self.disk_cache and self.cache_dir:\n",
        "                cache_key = hashlib.md5(str(t_tuple).encode()).hexdigest()\n",
        "                cache_file = os.path.join(self.cache_dir, f\"{cache_key}.pkl\")\n",
        "\n",
        "                if os.path.exists(cache_file):\n",
        "                    with open(cache_file, 'rb') as f:\n",
        "                        return pickle.load(f)\n",
        "\n",
        "            # Use in-memory cache or compute\n",
        "            result = memory_cache(t, n_sample)\n",
        "\n",
        "            # Save to disk cache if enabled\n",
        "            if self.disk_cache and self.cache_dir:\n",
        "                with open(cache_file, 'wb') as f:\n",
        "                    pickle.dump(result, f)\n",
        "\n",
        "            return result\n",
        "\n",
        "        return wrapped_func if self.cache_enabled else func"
      ],
      "metadata": {
        "id": "8QD7-28s6yz2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MnistModel(nn.Module):\n",
        "    def __init__(self, device=\"cpu\", embedding_size=0):\n",
        "        \"\"\"\n",
        "        A simple MNIST classifier.\n",
        "        If embedding_size > 0, the model expects an additional quantum embedding vector to be concatenated with the flattened image.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        input_size = 28 * 28\n",
        "        if embedding_size:\n",
        "            input_size += embedding_size\n",
        "        self.device = device\n",
        "        self.embedding_size = embedding_size\n",
        "        self.linear = nn.Linear(input_size, 10)\n",
        "\n",
        "    def forward(self, xb, emb=None):\n",
        "        xb = xb.view(-1, 784)\n",
        "        if self.embedding_size and emb is not None:\n",
        "            xb = torch.cat((xb, emb), dim=1)\n",
        "        out = self.linear(xb)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, emb=None):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        output = self(images, emb.to(self.device)) if self.embedding_size and emb is not None else self(images)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "        acc = accuracy(output, labels)\n",
        "        return loss, acc\n",
        "\n",
        "    def validation_step(self, batch, emb=None):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        output = self(images, emb.to(self.device)) if self.embedding_size and emb is not None else self(images)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "        acc = accuracy(output, labels)\n",
        "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x[\"val_loss\"] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_accs = [x[\"val_acc\"] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()\n",
        "        return {\"val_loss\": epoch_loss.item(), \"val_acc\": epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(f\"Epoch [{epoch}], val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\")\n",
        "        return result[\"val_loss\"], result[\"val_acc\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class HybridMnistModel(nn.Module):\n",
        "    def __init__(self, device=\"cpu\", embedding_size=0):\n",
        "        \"\"\"\n",
        "        A hybrid model that first extracts CNN features from the image and then\n",
        "        concatenates the quantum embedding.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Define a simple CNN feature extractor.\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),  # input: [B,1,28,28] -> [B,16,28,28]\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # -> [B,16,14,14]\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # -> [B,32,14,14]\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)   # -> [B,32,7,7]\n",
        "        )\n",
        "        cnn_output_size = 32 * 7 * 7\n",
        "        total_input_size = cnn_output_size + embedding_size\n",
        "        self.fc = nn.Linear(total_input_size, 10)\n",
        "        self.device = device\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "    def forward(self, image, emb=None):\n",
        "        # Process image with CNN.\n",
        "        features = self.cnn(image)  # Expecting image shape: [B,1,28,28]\n",
        "        features = features.view(features.size(0), -1)\n",
        "        if emb is not None:\n",
        "            features = torch.cat((features, emb), dim=1)\n",
        "        return self.fc(features)\n",
        "\n",
        "    def training_step(self, batch, emb=None):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        output = self(images, emb.to(self.device)) if self.embedding_size and emb is not None else self(images)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "        _, preds = torch.max(output, dim=1)\n",
        "        acc = torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "        return loss, acc\n",
        "\n",
        "    def validation_step(self, batch, emb=None):\n",
        "        images, labels = batch\n",
        "        images, labels = images.to(self.device), labels.to(self.device)\n",
        "        output = self(images, emb.to(self.device)) if self.embedding_size and emb is not None else self(images)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "        _, preds = torch.max(output, dim=1)\n",
        "        acc = torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "        return {\"val_loss\": loss, \"val_acc\": acc}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(f\"Epoch [{epoch}], val_loss: {result['val_loss']:.4f}, val_acc: {result['val_acc']:.4f}\")\n",
        "        return result[\"val_loss\"], result[\"val_acc\"]"
      ],
      "metadata": {
        "id": "3Szl9zpQTS_5"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "#####################\n",
        "#   DATA CONFIG     #\n",
        "#####################\n",
        "\n",
        "# previously src/config.py\n",
        "import yaml\n",
        "\n",
        "def load_config(config_path=\"yaml_default_config.yaml\"):\n",
        "    \"\"\"\n",
        "    Load configuration from the YAML file.\n",
        "    \"\"\"\n",
        "    base_dir = os.getcwd()\n",
        "    config_full_path = os.path.join(base_dir, config_path)\n",
        "    with open(config_full_path, \"r\") as f:\n",
        "        cfg = yaml.safe_load(f)\n",
        "    return cfg\n",
        "\n",
        "def create_scaleway_session(cfg):\n",
        "    \"\"\"\n",
        "    Create a Scaleway QaaS session based on the config and environment variables.\n",
        "    Expects SCW_PROJECT_ID and SCW_SECRET_KEY to be set in the environment.\n",
        "    \"\"\"\n",
        "    import perceval.providers.scaleway as scw\n",
        "    # Get credentials from the .env file using python-decouple.\n",
        "    proj_id = decouple_config(\"SCW_PROJECT_ID\") #\n",
        "    token = decouple_config(\"SCW_SECRET_KEY\") #\n",
        "    platform = cfg[\"quantum\"].get(\"scaleway_platform\", \"sim:sampling:p100\")\n",
        "    if not proj_id or not token:\n",
        "        raise RuntimeError(\"Scaleway credentials not found in environment.\")\n",
        "    session = scw.Session(\n",
        "        project_id=proj_id,\n",
        "        token=token,\n",
        "        platform=platform,\n",
        "        max_idle_duration_s=1200,\n",
        "        max_duration_s=3600\n",
        "    )\n",
        "    return session\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "#####################\n",
        "#   DATA UTILS      #\n",
        "#####################\n",
        "class MNIST_partial(Dataset):\n",
        "    def __init__(self, data=\"./data\", transform=None, split=\"train\"):\n",
        "        \"\"\"\n",
        "        Load the reduced MNIST dataset from CSV.\n",
        "        :param data: Path to folder containing train.csv and val.csv.\n",
        "        :param transform: Optional transform function.\n",
        "        :param split: \"train\" or \"val\"\n",
        "        \"\"\"\n",
        "        # Compute the base directory (directory of this file)\n",
        "        base_dir = os.getcwd()\n",
        "        # If 'data' is given as a relative path, make it absolute relative to this file:\n",
        "        self.data_dir = os.path.join(base_dir, data)\n",
        "        self.transform = transform\n",
        "        if split == \"train\":\n",
        "            filename = os.path.join(self.data_dir, \"train.csv\")\n",
        "        elif split == \"val\":\n",
        "            filename = os.path.join(self.data_dir, \"val.csv\")\n",
        "        else:\n",
        "            raise AttributeError(\"split must be 'train' or 'val'\")\n",
        "        self.df = pd.read_csv(filename)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df[\"image\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_str = self.df[\"image\"].iloc[idx]\n",
        "        label = int(self.df[\"label\"].iloc[idx])\n",
        "        img_list = re.split(r\",\", img_str)\n",
        "        img_list[0] = img_list[0][1:]\n",
        "        img_list[-1] = img_list[-1][:-1]\n",
        "        img_float = [float(el) for el in img_list]\n",
        "        # Reshape to (1, 28, 28)\n",
        "        img_tensor = torch.tensor(img_float).reshape(1, 28, 28)\n",
        "        if self.transform is not None:\n",
        "            img_tensor = self.transform(img_tensor)\n",
        "        return img_tensor, label\n",
        "\n",
        "def get_dataloader(dataset, batch_size, shuffle):\n",
        "    \"\"\"Return a DataLoader for the given dataset.\"\"\"\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "#####################\n",
        "# TRAINING UTILS    #\n",
        "#####################\n",
        "def plot_training_metrics(train_acc, val_acc, train_loss, val_loss):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    epochs = list(range(1, len(train_acc) + 1))\n",
        "    axes[0].plot(epochs, train_acc, label=\"Training Accuracy\")\n",
        "    axes[0].plot(epochs, val_acc, label=\"Validation Accuracy\")\n",
        "    axes[0].set_xlabel(\"Epochs\")\n",
        "    axes[0].set_ylabel(\"Accuracy\")\n",
        "    axes[0].set_title(\"Accuracy over Epochs\")\n",
        "    axes[0].legend()\n",
        "    axes[1].plot(epochs, train_loss, label=\"Training Loss\")\n",
        "    axes[1].plot(epochs, val_loss, label=\"Validation Loss\")\n",
        "    axes[1].set_xlabel(\"Epochs\")\n",
        "    axes[1].set_ylabel(\"Loss\")\n",
        "    axes[1].set_title(\"Loss over Epochs\")\n",
        "    axes[1].legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"training_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
        "\n",
        "\n",
        "def compute_downsample_shape(nb_params, target_aspect=1.0):\n",
        "    \"\"\"\n",
        "    Compute a downsample shape (height, width) such that height * width == nb_params if possible.\n",
        "    If not, choose the factor pair that is as close as possible to a square (or the target aspect ratio).\n",
        "    target_aspect=1.0 corresponds to a square.\n",
        "\n",
        "    Parameters:\n",
        "      nb_params: int - the desired total number of pixels (phase parameters)\n",
        "      target_aspect: float - desired aspect ratio (width/height), default 1.0\n",
        "\n",
        "    Returns:\n",
        "      (height, width): tuple of ints representing the downsample shape.\n",
        "    \"\"\"\n",
        "    from math import sqrt\n",
        "    # Try to find factor pairs exactly.\n",
        "    for h in range(int(sqrt(nb_params)), 0, -1):\n",
        "        if nb_params % h == 0:\n",
        "            w = nb_params // h\n",
        "            return h, w\n",
        "    # If no factor pair exactly divides nb_params, default to a square shape.\n",
        "    h = int(sqrt(nb_params))\n",
        "    return h, h\n",
        "\n",
        "\n",
        "\n",
        "######################\n",
        "# PCA Downsampling   #\n",
        "######################\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def pca_downsample(image_tensor, pca_model):\n",
        "    \"\"\"\n",
        "    Downsample a 28x28 image using a pre-trained PCA model.\n",
        "\n",
        "    Parameters:\n",
        "      image_tensor: torch tensor of shape [1, 28, 28] (or [28,28])\n",
        "      pca_model: a scikit-learn PCA model pre-trained on flattened MNIST images\n",
        "\n",
        "    Returns:\n",
        "      A torch tensor of shape [pca_model.n_components]\n",
        "    \"\"\"\n",
        "    # Ensure the image is in 2D: if [1,28,28], remove the channel dimension.\n",
        "    if image_tensor.dim() == 3 and image_tensor.shape[0] == 1:\n",
        "        image_tensor = image_tensor.squeeze(0)\n",
        "    # Flatten the image (28*28)\n",
        "    img_flat = image_tensor.view(-1).cpu().numpy()\n",
        "    # Project the flattened image onto the PCA components\n",
        "    reduced = pca_model.transform([img_flat])[0]\n",
        "    # Convert back to a torch tensor\n",
        "    return torch.tensor(reduced, dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch.fft\n",
        "\n",
        "def fourier_encode(image_tensor, n_features=100):\n",
        "    \"\"\"\n",
        "    Computes Fourier features from an image tensor.\n",
        "\n",
        "    Parameters:\n",
        "      image_tensor: A tensor of shape [1, 28, 28] (or [28,28]).\n",
        "      n_features  : Number of Fourier features to extract.\n",
        "\n",
        "    Returns:\n",
        "      A 1D tensor of Fourier features (absolute values of the FFT).\n",
        "    \"\"\"\n",
        "    # Ensure the image is 2D.\n",
        "    if image_tensor.dim() == 3 and image_tensor.shape[0] == 1:\n",
        "        image_tensor = image_tensor.squeeze(0)\n",
        "    fft_result = torch.fft.fft2(image_tensor)\n",
        "    fft_abs = torch.abs(fft_result).flatten()\n",
        "    # Select the first n_features; you might also choose to sort or use a different selection method.\n",
        "    return fft_abs[:n_features]\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N0E1u8HeTVuz"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import yaml\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# A simple PCA-based classifier\n",
        "class ClassicPCAClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64, num_classes=10):\n",
        "        super(ClassicPCAClassifier, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Function to load and prepare data (flattening the images)\n",
        "def prepare_pca_data(dataset):\n",
        "    data_list, labels_list = [], []\n",
        "    for img, label in dataset:\n",
        "        data_list.append(img.view(-1).numpy())\n",
        "        labels_list.append(label)\n",
        "    return np.array(data_list), np.array(labels_list)\n",
        "\n",
        "# Minimal main for classical baseline\n",
        "def main_classic():\n",
        "    result_dir = f\"results/classical_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    os.makedirs(result_dir, exist_ok=True)\n",
        "    print(f\"Results will be saved to: {result_dir}\")\n",
        "\n",
        "    # Load datasets from CSV files in \"data\" folder\n",
        "    train_dataset = MNIST_partial(data=\"data\", split=\"train\")\n",
        "    val_dataset = MNIST_partial(data=\"data\", split=\"val\")\n",
        "\n",
        "    # Prepare data for PCA\n",
        "    X_train, y_train = prepare_pca_data(train_dataset)\n",
        "    X_val, y_val = prepare_pca_data(val_dataset)\n",
        "\n",
        "    # For matching quantum parameter count, suppose n_components is:\n",
        "    m = cfg[\"quantum\"][\"n_modes\"]\n",
        "    n_components = m * (m - 1) - (m // 2)\n",
        "    print(f\"Using PCA with {n_components} components\")\n",
        "\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_val_pca = pca.transform(X_val)\n",
        "\n",
        "    # Build datasets for PCA features\n",
        "    class PCADataset(torch.utils.data.Dataset):\n",
        "        def __init__(self, X, y):\n",
        "            self.X = torch.tensor(X, dtype=torch.float32)\n",
        "            self.y = torch.tensor(y, dtype=torch.long)\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "        def __getitem__(self, idx):\n",
        "            return self.X[idx], self.y[idx]\n",
        "\n",
        "    train_dataset_pca = PCADataset(X_train_pca, y_train)\n",
        "    val_dataset_pca = PCADataset(X_val_pca, y_val)\n",
        "\n",
        "    batch_size = cfg[\"data\"][\"batch_size\"]\n",
        "    train_loader = DataLoader(train_dataset_pca, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset_pca, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Initialize classical model\n",
        "    model = ClassicPCAClassifier(input_dim=n_components, hidden_dim=64, num_classes=10).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg[\"training\"][\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    epochs = cfg[\"training\"][\"epochs\"]\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss, running_corrects, num_samples = 0.0, 0, 0\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects += torch.sum(preds == labels).item()\n",
        "            num_samples += inputs.size(0)\n",
        "        epoch_loss = running_loss / num_samples\n",
        "        epoch_acc = running_corrects / num_samples\n",
        "        print(f\"Epoch {epoch+1} - Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "    # (Optional: add evaluation and visualizations, e.g. confusion matrix, PCA component plots, etc.)\n",
        "    # Save the PCA model for later use\n",
        "    with open(os.path.join(\"data\", \"pca_model.pkl\"), \"wb\") as f:\n",
        "        pickle.dump(pca, f)\n",
        "    print(\"PCA model saved.\")\n",
        "\n",
        "main_classic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53XZPR-HTeSi",
        "outputId": "e986436d-7579-4c3e-8da8-c74c94ae18a7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results will be saved to: results/classical_run_20250331_175823\n",
            "Using PCA with 540 components\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 200/200 [00:01<00:00, 194.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 1.3210, Acc: 0.7633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|██████████| 200/200 [00:00<00:00, 589.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Loss: 0.3654, Acc: 0.9173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|██████████| 200/200 [00:00<00:00, 594.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Loss: 0.2334, Acc: 0.9407\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|██████████| 200/200 [00:00<00:00, 585.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Loss: 0.1734, Acc: 0.9557\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|██████████| 200/200 [00:00<00:00, 555.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Loss: 0.1315, Acc: 0.9698\n",
            "PCA model saved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Begin Minimal Hybrid Main ---\n",
        "def main_quantum():\n",
        "    print(\"Loaded inline configuration:\")\n",
        "    print(cfg)\n",
        "\n",
        "    # For a local simulation, we do not create a Scaleway session.\n",
        "    session = None\n",
        "\n",
        "    circuit_type = cfg[\"quantum\"].get(\"circuit_type\", \"triangular\")\n",
        "    if circuit_type == \"pdf\":\n",
        "        builder = PdfInterferometerBuilder()\n",
        "    elif circuit_type == \"base\":\n",
        "        builder = BaseInterferometerBuilder()\n",
        "    elif circuit_type == \"triangular\":\n",
        "        builder = TriangularInterferometerBuilder()\n",
        "    elif circuit_type == \"rectangular\":\n",
        "        builder = RectangularInterferometerBuilder()\n",
        "    elif circuit_type == \"convolutional\":\n",
        "        # Define random filters for demonstration.\n",
        "        filterA = np.random.rand(6, 5)\n",
        "        filterB = np.random.rand(6, 5)\n",
        "        builder = ConvolutionalInterferometerBuilder(filterA, filterB, image_size=(28,28))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported circuit type: {circuit_type}\")\n",
        "\n",
        "    # Instantiate the boson sampler (choose variational or not)\n",
        "    if cfg[\"quantum\"].get(\"variational\", False):\n",
        "        bs = VariationalBosonSampler(\n",
        "            m=cfg[\"quantum\"][\"n_modes\"],\n",
        "            n=cfg[\"quantum\"][\"n_photons\"],\n",
        "            postselect=cfg[\"quantum\"][\"postselect\"],\n",
        "            backend=cfg[\"quantum\"][\"local_backend\"],\n",
        "            session=session,\n",
        "            builder=builder,\n",
        "            cache_enabled=cfg[\"quantum\"].get(\"cache_enabled\", False),\n",
        "            cache_directory=cfg[\"quantum\"].get(\"cache_directory\", \"results/cache\")\n",
        "        )\n",
        "    else:\n",
        "        bs = BosonSampler(\n",
        "            m=cfg[\"quantum\"][\"n_modes\"],\n",
        "            n=cfg[\"quantum\"][\"n_photons\"],\n",
        "            postselect=cfg[\"quantum\"][\"postselect\"],\n",
        "            backend=cfg[\"quantum\"][\"local_backend\"],\n",
        "            session=session,\n",
        "            builder=builder,\n",
        "            cache_enabled=cfg[\"quantum\"].get(\"cache_enabled\", False),\n",
        "            cache_directory=cfg[\"quantum\"].get(\"cache_directory\", \"results/cache\")\n",
        "        )\n",
        "    bs.n_samples = cfg[\"quantum\"][\"n_samples\"]\n",
        "\n",
        "    print(\"BosonSampler embedding size:\", bs.embedding_size)\n",
        "\n",
        "    # Load dataset (assume MNIST CSVs are in the \"data\" folder)\n",
        "    train_dataset = MNIST_partial(data=\"data\", split=\"train\")\n",
        "    val_dataset = MNIST_partial(data=\"data\", split=\"val\")\n",
        "    train_loader = get_dataloader(train_dataset, batch_size=cfg[\"data\"][\"batch_size\"], shuffle=cfg[\"data\"][\"shuffle\"])\n",
        "    val_loader = get_dataloader(val_dataset, batch_size=cfg[\"data\"][\"batch_size\"], shuffle=False)\n",
        "\n",
        "    # Choose device\n",
        "    device = \"cuda\" if (cfg[\"training\"][\"device\"]==\"auto\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    # Initialize the model (Hybrid model uses the quantum embedding)\n",
        "    net = HybridMnistModel(device=device, embedding_size=bs.embedding_size)\n",
        "    net = net.to(device)\n",
        "\n",
        "    # Setup optimizer (include boson sampler parameters if variational)\n",
        "    optimizer_name = cfg[\"training\"][\"optimizer\"].lower()\n",
        "    if cfg[\"quantum\"].get(\"variational\", False):\n",
        "        params_to_opt = list(net.parameters()) + [bs.params]\n",
        "    else:\n",
        "        params_to_opt = net.parameters()\n",
        "\n",
        "    if optimizer_name == \"adam\":\n",
        "        optimizer = torch.optim.Adam(params_to_opt, lr=cfg[\"training\"][\"learning_rate\"])\n",
        "    elif optimizer_name == \"sgd\":\n",
        "        optimizer = torch.optim.SGD(params_to_opt, lr=cfg[\"training\"][\"learning_rate\"])\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported optimizer specified in config.\")\n",
        "\n",
        "    # Train the model\n",
        "    history = fit(\n",
        "        epochs=cfg[\"training\"][\"epochs\"],\n",
        "        lr=cfg[\"training\"][\"learning_rate\"],\n",
        "        model=net,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        bs=bs,\n",
        "        optimizer=optimizer,\n",
        "        cfg=cfg\n",
        "    )\n",
        "    print(\"Training completed.\")\n",
        "\n",
        "main_quantum()\n",
        "# --- End Minimal Hybrid Main ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhckZnZ8TZ8h",
        "outputId": "c95e7fdb-7c0a-4f29-e605-c1d53486f833"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded inline configuration:\n",
            "{'data': {'train_csv': 'data/train.csv', 'test_csv': 'data/val.csv', 'batch_size': 30, 'shuffle': True}, 'model': {'type': 'hybrid', 'fourier_features': 50, 'conv_channels': 16, 'kernel_size': 5, 'output_dim': 30}, 'quantum': {'backend': 'local', 'n_modes': 24, 'n_photons': 10, 'postselect': 3, 'n_samples': 1000, 'circuit_type': 'pdf', 'variational': True, 'adaptive_sampling': False, 'cache_enabled': True, 'cache_directory': 'results/cache', 'local_backend': 'SLOS', 'scaleway_platform': 'sim:sampling:2l4', 'downsample_method': 'pca'}, 'training': {'epochs': 5, 'learning_rate': 0.001, 'optimizer': 'adam', 'device': 'auto'}, 'optimization': {'enabled': True, 'method': 'tpe', 'max_evals': 10, 'space': {'n_modes': [6, 12, 24, 30], 'n_photons': [6, 10], 'learning_rate': [0.0001, 0.001, 0.01]}}}\n",
            "BosonSampler embedding size: 4540085\n",
            "Using device: cuda\n",
            "Downsample shape for images set to: 20x27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/200 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXWWOIBVXVJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LZ0AlxU5TcQ6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h7lAIJsHTX0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}